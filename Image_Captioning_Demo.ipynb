{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9982b6",
   "metadata": {},
   "source": [
    "# 🖼️ Image Captioning \n",
    "\n",
    "This notebook demonstrates the end-to-end workflow for the CNN + Attention LSTM image captioning project contained in this repository.\n",
    "\n",
    "It will: \n",
    "1. Inspect environment & dependencies\n",
    "2. (Optionally) locate / download dataset (Kaggle mini COCO) or fall back to a tiny synthetic example\n",
    "3. Build / load vocabulary\n",
    "4. Construct the encoder-decoder model and load an existing checkpoint if available\n",
    "5. (Optional) Run a super-light illustrative training micro-step (on 1–2 batches)\n",
    "6. Run inference to generate captions for sample images\n",
    "7. Provide next steps & improvement ideas\n",
    "\n",
    "> Designed to be resilient: if the full dataset isn't present or Kaggle credentials aren't configured, the notebook still runs using a synthetic mini dataset so you can exercise the pipeline quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83c097",
   "metadata": {},
   "source": [
    "## 1. Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f51531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, random, math, time, pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "print('Python:', sys.version)\n",
    "print('Torch version:', torch.__version__)\n",
    "print('Torchvision version:', torchvision.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('MPS available:', torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False)\n",
    "\n",
    "# Add project root to path (in case notebook run from a different working dir)\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()\n",
    "if (PROJECT_ROOT / 'image_captioning').exists():\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "print('Project root:', PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c19f24",
   "metadata": {},
   "source": [
    "## 2.  Download Dataset\n",
    "This project expects a COCO-style (or simplified list) captions JSON and an `images/` directory.\n",
    "\n",
    "If you already trained via `scripts/train.py`, artifacts should be present under `artifacts/`.\n",
    "**Kaggle Download (Optional):** In a standard environment you would run the CLI script:\n",
    "```bash\n",
    "python scripts/train.py --epochs 1 --no-pretrained --batch-size 8\n",
    "```\n",
    "For portability (e.g. in hosted notebook environments without Kaggle credentials), we skip automatic download here and attempt to reuse existing local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23866175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dataset found; will synthesize a tiny in-memory dataset for demonstration.\n"
     ]
    }
   ],
   "source": [
    "if 'PROJECT_ROOT' not in globals():\n",
    "    from pathlib import Path\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "DATA_ROOT_CANDIDATES = [\n",
    "    PROJECT_ROOT / 'data',\n",
    "    PROJECT_ROOT / 'dataset',\n",
    "    PROJECT_ROOT / 'mini_coco',\n",
    "    PROJECT_ROOT / 'kaggle',\n",
    "] + [p for p in (PROJECT_ROOT).glob('*') if p.is_dir() and 'coco' in p.name.lower()]\n",
    "\n",
    "captions_file = None\n",
    "images_dir = None\n",
    "for root in DATA_ROOT_CANDIDATES:\n",
    "    if not root.exists():\n",
    "        continue\n",
    "    cand_caps = list(root.glob('**/captions*.json'))\n",
    "    cand_imgs = [p for p in root.glob('**/images') if p.is_dir()]\n",
    "    if cand_caps and cand_imgs:\n",
    "        captions_file = cand_caps[0]\n",
    "        images_dir = cand_imgs[0]\n",
    "        break\n",
    "\n",
    "if captions_file and images_dir:\n",
    "    print('Found dataset:')\n",
    "    print('  Captions JSON:', captions_file)\n",
    "    print('  Images dir  :', images_dir)\n",
    "else:\n",
    "    print(' dataset found; will synthesize a tiny in-memory dataset for demonstration.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a7104",
   "metadata": {},
   "source": [
    "## 3. Vocabulary Build / Load\n",
    "If a vocabulary already exists in `artifacts/vocab.json` we load it. Otherwise we build one from either the located captions file or a synthetic fallback set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_captioning.utils.tokenizer import Vocabulary, tokenize\n",
    "\n",
    "artifacts_dir = PROJECT_ROOT / 'artifacts'\n",
    "vocab_path = artifacts_dir / 'vocab.json'\n",
    "vocab = None\n",
    "\n",
    "def build_vocab_from_captions(captions_list, min_freq=1, max_size=10000):\n",
    "    freqs = {}\n",
    "    for c in captions_list:\n",
    "        for tok in tokenize(c):\n",
    "            freqs[tok] = freqs.get(tok, 0) + 1\n",
    "    # Sort by frequency desc then alpha\n",
    "    tokens = [t for t,_ in sorted(freqs.items(), key=lambda x: (-x[1], x[0])) if freqs[t] >= min_freq]\n",
    "    if max_size: tokens = tokens[:max_size]\n",
    "    return Vocabulary.from_tokens(tokens)\n",
    "\n",
    "if vocab_path.exists():\n",
    "    vocab = Vocabulary.load(vocab_path)\n",
    "    print('Loaded existing vocabulary with size:', len(vocab))\n",
    "else:\n",
    "    captions_list = []\n",
    "    if 'captions_file' in globals() and captions_file and captions_file.exists():\n",
    "        try:\n",
    "            payload = json.loads(captions_file.read_text())\n",
    "            if isinstance(payload, dict) and 'annotations' in payload:\n",
    "                annotations = payload['annotations']\n",
    "            elif isinstance(payload, list):\n",
    "                annotations = payload\n",
    "            else:\n",
    "                annotations = []\n",
    "            for ann in annotations[:2000]:  # sample subset\n",
    "                cap = ann.get('caption') or ann.get('text') or ''\n",
    "                if cap:\n",
    "                    captions_list.append(cap)\n",
    "        except Exception as e:\n",
    "            print('Failed to parse real captions, fallback to synthetic. Error:', e)\n",
    "    if not captions_list:\n",
    "        captions_list = [\n",
    "            'a dog playing with a ball',\n",
    "            'a cat sitting on the mat',\n",
    "            'a child riding a red bicycle',\n",
    "            'a group of people hiking a mountain trail',\n",
    "            'a plate of fresh colorful fruit'\n",
    "        ]\n",
    "    vocab = build_vocab_from_captions(captions_list)\n",
    "    artifacts_dir.mkdir(exist_ok=True, parents=True)\n",
    "    vocab.save(vocab_path)\n",
    "    print('Built vocabulary size:', len(vocab))\n",
    "\n",
    "# Quick inspection\n",
    "print('Sample tokens:', list(vocab.token_to_idx.keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05858f5c",
   "metadata": {},
   "source": [
    "## 4. Create / Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_captioning.config import ModelConfig\n",
    "from image_captioning.models.encoder_decoder import EncoderDecoder\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "model_cfg = ModelConfig(vocab_size=len(vocab), embed_dim=256, hidden_dim=512, attention_dim=256, num_layers=1, dropout=0.1, use_pretrained=False)\n",
    "model = EncoderDecoder(model_cfg, vocab).to(device)\n",
    "\n",
    "checkpoint_dir = artifacts_dir / 'checkpoints'\n",
    "loaded_checkpoint = None\n",
    "if checkpoint_dir.exists():\n",
    "    # Pick latest epoch numerically\n",
    "    checkpoints = sorted(checkpoint_dir.glob('model_epoch_*.pt'))\n",
    "    if checkpoints:\n",
    "        latest = checkpoints[-1]\n",
    "        try:\n",
    "            state = torch.load(latest, map_location=device)\n",
    "            if isinstance(state, dict) and 'model_state' in state:\n",
    "                model.load_state_dict(state['model_state'])\n",
    "            else:\n",
    "                model.load_state_dict(state)\n",
    "            loaded_checkpoint = latest\n",
    "            print(f'Loaded checkpoint: {latest.name}')\n",
    "        except Exception as e:\n",
    "            print('Could not load checkpoint:', e)\n",
    "else:\n",
    "    print('No checkpoints directory found; using fresh model.')\n",
    "\n",
    "print('Model parameters:', sum(p.numel() for p in model.parameters())//1000, 'K')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bc6973",
   "metadata": {},
   "source": [
    "## 5. Micro Training (Illustrative)\n",
    "Skips entirely if no real dataset is present. Trains on 1–2 batches only to demonstrate the API (won't produce quality captions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from image_captioning.data.dataset import CocoCaptionsDataset, CaptionExample, collate_fn\n",
    "\n",
    "def build_demo_dataloader(max_items=32):\n",
    "    if captions_file and images_dir and captions_file.exists() and images_dir.exists():\n",
    "        try:\n",
    "            ds = CocoCaptionsDataset(captions_file=str(captions_file), images_root=str(images_dir), vocab=vocab, transform=T.Compose([T.Resize((224,224)), T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])]))\n",
    "            if len(ds) > max_items:\n",
    "                # Subsample deterministically\n",
    "                indices = list(range(len(ds)))[:max_items]\n",
    "                ds.examples = [ds.examples[i] for i in indices]\n",
    "            return DataLoader(ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "        except Exception as e:\n",
    "            print('Failed constructing real dataloader:', e)\n",
    "    # Fallback synthetic dataset (solid color images in-memory)\n",
    "    examples = []\n",
    "    colors = [(255,0,0), (0,255,0), (0,0,255), (200,200,0)]\n",
    "    captions = ['a red square', 'a green square', 'a blue square', 'a yellow square']\n",
    "    for i,(c,cap) in enumerate(zip(colors, captions)):\n",
    "        img_path = artifacts_dir / f'synthetic_{i}.png'\n",
    "        if not img_path.exists():\n",
    "            im = Image.new('RGB', (224,224), c)\n",
    "            im.save(img_path)\n",
    "        examples.append(CaptionExample(str(img_path), cap))\n",
    "    ds = CocoCaptionsDataset(captions_file=None, images_root=str(artifacts_dir), vocab=vocab, transform=T.Compose([T.Resize((224,224)), T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])]))\n",
    "    ds.examples = examples\n",
    "    return DataLoader(ds, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "micro_loader = build_demo_dataloader()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.token_to_idx.get('<pad>', 0))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "max_steps = 2\n",
    "step = 0\n",
    "losses = []\n",
    "for batch in micro_loader:\n",
    "    images = batch['images'].to(device)\n",
    "    captions = batch['captions'].to(device)\n",
    "    targets = captions[:,1:]  # shift\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images, captions[:,:-1])  # predict next token\n",
    "    outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    step += 1\n",
    "    print(f'Step {step} loss: {loss.item():.3f}')\n",
    "    if step >= max_steps: break\n",
    "print('Micro training complete. Avg loss:', sum(losses)/len(losses))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadf375",
   "metadata": {},
   "source": [
    "## 6. Inference / Caption Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_captioning.inference.service import CaptionGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "generator = CaptionGenerator(model=model, vocab=vocab, device=device)\n",
    "\n",
    "# Pick an image: prefer real dataset image, else synthetic\n",
    "candidate_images = []\n",
    "if 'images_dir' in globals() and images_dir and images_dir.exists():\n",
    "    candidate_images = list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.png'))\n",
    "if not candidate_images:\n",
    "    candidate_images = list(artifacts_dir.glob('synthetic_*.png'))\n",
    "assert candidate_images, 'No images found to caption.'\n",
    "sample_image = candidate_images[0]\n",
    "print('Using image:', sample_image)\n",
    "caption = generator.caption_image(str(sample_image), max_len=20)\n",
    "print('Generated caption:', caption)\n",
    "\n",
    "# Display image & caption\n",
    "img_display = Image.open(sample_image).convert('RGB')\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(img_display)\n",
    "plt.axis('off')\n",
    "plt.title(caption)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1f84f",
   "metadata": {},
   "source": [
    "## 7. Next Steps & Improvements\n",
    "**Potential Enhancements:**\n",
    "- Implement beam search decoding for higher-quality captions.\n",
    "- Add evaluation metrics (BLEU, CIDEr, ROUGE, METEOR).\n",
    "- Increase training epochs & optionally enable pretrained encoder weights.\n",
    "- Add scheduled sampling or label smoothing to improve robustness.\n",
    "- Visualize attention maps over image regions per generated token.\n",
    "- Experiment with transformer-based decoder for comparison.\n",
    "\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
